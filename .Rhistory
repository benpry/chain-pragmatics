mutate(prompt_type = "basic")
)
top_30_familiar <- slice_max(df.differences, order_by=FAM, n=30)
bottom_30_familiar <- slice_min(df.differences, order_by=FAM, n=30)
mean_fam_difference = mean(top_30_familiar$appropriateness_rationale, na.rm=T) - mean(bottom_30_familiar$appropriateness_rationale, na.rm=T)
prompt_type = "non_explanation"
K = 10
temp = 0.9
df.responses_rationale <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_basic <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, "basic", K, temp)))
df.differences <- df.responses_rationale |>
select(ID, appropriateness_score) |>
rename(appropriateness_rationale = appropriateness_score) |>
left_join(df.responses_basic, on=ID) |>
rename(appropriateness_basic = appropriateness_score) |>
mutate(appropriateness_diff = appropriateness_rationale - appropriateness_basic) |>
mutate(appropriateness_rationale_norm = appropriateness_rationale - 2.5)
df.concatenated <- df.responses_rationale |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "rationale") |>
rbind(df.responses_basic |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "basic")
)
top_30_familiar <- slice_max(df.differences, order_by=FAM, n=30)
bottom_30_familiar <- slice_min(df.differences, order_by=FAM, n=30)
mean_fam_difference = mean(top_30_familiar$appropriateness_rationale, na.rm=T) - mean(bottom_30_familiar$appropriateness_rationale, na.rm=T)
prompt_type = "basic"
K = 10
temp = 0.9
df.responses_rationale <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_basic <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, "basic", K, temp)))
df.differences <- df.responses_rationale |>
select(ID, appropriateness_score) |>
rename(appropriateness_rationale = appropriateness_score) |>
left_join(df.responses_basic, on=ID) |>
rename(appropriateness_basic = appropriateness_score) |>
mutate(appropriateness_diff = appropriateness_rationale - appropriateness_basic) |>
mutate(appropriateness_rationale_norm = appropriateness_rationale - 2.5)
df.concatenated <- df.responses_rationale |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "rationale") |>
rbind(df.responses_basic |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "basic")
)
top_30_familiar <- slice_max(df.differences, order_by=FAM, n=30)
bottom_30_familiar <- slice_min(df.differences, order_by=FAM, n=30)
mean_fam_difference = mean(top_30_familiar$appropriateness_rationale, na.rm=T) - mean(bottom_30_familiar$appropriateness_rationale, na.rm=T)
gpt_version = "curie"
prompt_type = "basic"
K = 10
temp = 0.9
df.responses_rationale <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_basic <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, "basic", K, temp)))
df.differences <- df.responses_rationale |>
select(ID, appropriateness_score) |>
rename(appropriateness_rationale = appropriateness_score) |>
left_join(df.responses_basic, on=ID) |>
rename(appropriateness_basic = appropriateness_score) |>
mutate(appropriateness_diff = appropriateness_rationale - appropriateness_basic) |>
mutate(appropriateness_rationale_norm = appropriateness_rationale - 2.5)
df.concatenated <- df.responses_rationale |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "rationale") |>
rbind(df.responses_basic |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "basic")
)
top_30_familiar <- slice_max(df.differences, order_by=FAM, n=30)
bottom_30_familiar <- slice_min(df.differences, order_by=FAM, n=30)
mean_fam_difference = mean(top_30_familiar$appropriateness_rationale, na.rm=T) - mean(bottom_30_familiar$appropriateness_rationale, na.rm=T)
gpt_version = "curie"
prompt_type = "basic"
K = 10
temp = 0.9
df.responses_rationale <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_basic <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, "basic", K, temp)))
prompt_type = "non_explanation"
K = 10
temp = 0.9
df.responses_rationale <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_basic <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, "basic", K, temp)))
df.differences <- df.responses_rationale |>
select(ID, appropriateness_score) |>
rename(appropriateness_rationale = appropriateness_score) |>
left_join(df.responses_basic, on=ID) |>
rename(appropriateness_basic = appropriateness_score) |>
mutate(appropriateness_diff = appropriateness_rationale - appropriateness_basic) |>
mutate(appropriateness_rationale_norm = appropriateness_rationale - 2.5)
df.concatenated <- df.responses_rationale |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "rationale") |>
rbind(df.responses_basic |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "basic")
)
top_30_familiar <- slice_max(df.differences, order_by=FAM, n=30)
bottom_30_familiar <- slice_min(df.differences, order_by=FAM, n=30)
mean_fam_difference = mean(top_30_familiar$appropriateness_rationale, na.rm=T) - mean(bottom_30_familiar$appropriateness_rationale, na.rm=T)
prompt_type = "QUD_v3"
K = 10
temp = 0.9
df.responses_rationale <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_basic <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, "basic", K, temp)))
df.differences <- df.responses_rationale |>
select(ID, appropriateness_score) |>
rename(appropriateness_rationale = appropriateness_score) |>
left_join(df.responses_basic, on=ID) |>
rename(appropriateness_basic = appropriateness_score) |>
mutate(appropriateness_diff = appropriateness_rationale - appropriateness_basic) |>
mutate(appropriateness_rationale_norm = appropriateness_rationale - 2.5)
df.concatenated <- df.responses_rationale |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "rationale") |>
rbind(df.responses_basic |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "basic")
)
top_30_familiar <- slice_max(df.differences, order_by=FAM, n=30)
bottom_30_familiar <- slice_min(df.differences, order_by=FAM, n=30)
mean_fam_difference = mean(top_30_familiar$appropriateness_rationale, na.rm=T) - mean(bottom_30_familiar$appropriateness_rationale, na.rm=T)
prompt_type = "similarity"
K = 10
temp = 0.9
df.responses_rationale <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_basic <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, "basic", K, temp)))
df.differences <- df.responses_rationale |>
select(ID, appropriateness_score) |>
rename(appropriateness_rationale = appropriateness_score) |>
left_join(df.responses_basic, on=ID) |>
rename(appropriateness_basic = appropriateness_score) |>
mutate(appropriateness_diff = appropriateness_rationale - appropriateness_basic) |>
mutate(appropriateness_rationale_norm = appropriateness_rationale - 2.5)
df.concatenated <- df.responses_rationale |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "rationale") |>
rbind(df.responses_basic |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "basic")
)
top_30_familiar <- slice_max(df.differences, order_by=FAM, n=30)
bottom_30_familiar <- slice_min(df.differences, order_by=FAM, n=30)
mean_fam_difference = mean(top_30_familiar$appropriateness_rationale, na.rm=T) - mean(bottom_30_familiar$appropriateness_rationale, na.rm=T)
prompt_type = "contrast"
K = 10
temp = 0.9
df.responses_rationale <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_basic <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, "basic", K, temp)))
df.differences <- df.responses_rationale |>
select(ID, appropriateness_score) |>
rename(appropriateness_rationale = appropriateness_score) |>
left_join(df.responses_basic, on=ID) |>
rename(appropriateness_basic = appropriateness_score) |>
mutate(appropriateness_diff = appropriateness_rationale - appropriateness_basic) |>
mutate(appropriateness_rationale_norm = appropriateness_rationale - 2.5)
df.concatenated <- df.responses_rationale |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "rationale") |>
rbind(df.responses_basic |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "basic")
)
top_30_familiar <- slice_max(df.differences, order_by=FAM, n=30)
bottom_30_familiar <- slice_min(df.differences, order_by=FAM, n=30)
mean_fam_difference = mean(top_30_familiar$appropriateness_rationale, na.rm=T) - mean(bottom_30_familiar$appropriateness_rationale, na.rm=T)
prompt_type = "contrast"
K = 10
temp = 0.9
df.responses_rationale <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_basic <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, "basic", K, temp)))
df.differences <- df.responses_rationale |>
select(ID, appropriateness_score) |>
rename(appropriateness_rationale = appropriateness_score) |>
left_join(df.responses_basic, on=ID) |>
rename(appropriateness_basic = appropriateness_score) |>
mutate(appropriateness_diff = appropriateness_rationale - appropriateness_basic) |>
mutate(appropriateness_rationale_norm = appropriateness_rationale - 2.5)
df.concatenated <- df.responses_rationale |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "rationale") |>
rbind(df.responses_basic |>
select(ID, appropriateness_score) |>
mutate(prompt_type = "basic")
)
top_30_familiar <- slice_max(df.differences, order_by=FAM, n=30)
bottom_30_familiar <- slice_min(df.differences, order_by=FAM, n=30)
mean_fam_difference = mean(top_30_familiar$appropriateness_rationale, na.rm=T) - mean(bottom_30_familiar$appropriateness_rationale, na.rm=T)
psycholing_cols <- c("CMP","ESI","MET","MGD","IMG","IMS","IMP","FAM","SRL","ALT")
model <- brm(appropriateness_rationale ~ FAM, family=cumulative(), data=df.differences)
library(tidyverse)
library(brms)
library(here)
library(broom)
corpus = "katz"
corpus_set = "test"
gpt_version = "curie"
prompt_types = c("basic", "non_explanation", "QUD", "similarity", "contrast")
K = 10
temp = 0.9
df.all_responses <- data.frame
for (prompt_type in prompt_types) {
df.responses_prompt <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.all_responses <- rbind(df.all_responses, df.responses_prompt)
}
View(df.all_responses)
df.all_responses <- data.frame()
for (prompt_type in prompt_types) {
df.responses_prompt <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.all_responses <- rbind(df.all_responses, df.responses_prompt)
}
prompt_types = c("basic", "non_explanation", "QUD_v3", "similarity", "contrast")
K = 10
temp = 0.9
df.all_responses <- data.frame()
for (prompt_type in prompt_types) {
df.responses_prompt <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.all_responses <- rbind(df.all_responses, df.responses_prompt)
}
print(df.all_responses)
View(df.all_responses)
df.all_responses <- data.frame()
for (prompt_type in prompt_types) {
df.responses_prompt <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_prompt <- df.responses_prompt |>
mutate(prompt_type = prompt_type)
df.all_responses <- rbind(df.all_responses, df.responses_prompt)
}
df.all_responses <- df.all_responses |>
select(ID, Group, Statement, prompt_type, appropriateness_score)
print(df.all_responses)
View(df.all_responses)
View(df.all_responses)
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
)
library(tidyverse)
library(brms)
library(here)
library(broom)
# constants
corpus = "katz"
corpus_set = "test"
gpt_version = "curie"
prompt_types = c("basic", "non_explanation", "QUD_v3", "similarity", "contrast")
K = 10
temp = 0.9
df.all_responses <- data.frame()
for (prompt_type in prompt_types) {
df.responses_prompt <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_prompt <- df.responses_prompt |>
mutate(prompt_type = prompt_type)
df.all_responses <- rbind(df.all_responses, df.responses_prompt)
}
df.all_responses <- df.all_responses |>
select(ID, Group, Statement, prompt_type, appropriateness_score)
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
)
View(df.responses_prompt)
View(df.responses_wide)
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
) |>
drop_na()
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
) |>
drop_na() |>
cor()
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
) |>
drop_na() |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
cor()
View(df.responses_wide)
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
) |>
drop_na()
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
cor()
View(corr_mat)
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
cor(method="pearson")
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
)
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
cor(method="pearson")
library(Hmisc)
install.packages("Hmisc")
library(Hmisc)
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
rcorr(method="pearson")
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
rcorr()
View(corr_mat)
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
rcorr(type="pearson")
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
) |>
drop_na()
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
rcorr(type="pearson")
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
rcorr(type="pearson")
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
rcorr(type="pearson")
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
) |>
drop_na()
View(df.responses_prompt)
View(df.responses_wide)
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
rcorr(type="pearson")
corrs, ps <- df.responses_wide |>
df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
rcorr(type="pearson")
df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
rcorr()
df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
as.matrix() |>
rcorr(method = "pearson")
df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
as.matrix() |>
rcorr(type = "pearson")
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
as.matrix() |>
rcorr(type = "pearson")
View(corr_mat)
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
)
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
as.matrix() |>
rcorr(type = "pearson")
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
)
corr_mat <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
as.matrix() |>
rcorr(type = "pearson")
View(corr_mat)
View(corr_mat)
r, n, p <- df.responses_wide |>
rs = corr_results[["r"]]
ns = corr_results[["n"]]
ps = corr_results[["p"]]
corr_results <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
as.matrix() |>
rcorr(type = "pearson")
rs = corr_results[["r"]]
ns = corr_results[["n"]]
ps = corr_results[["p"]]
View(rs)
View(corr_mat)
ps = corr_results[["P"]]
View(ps)
View(ns)
cor(df.responses_wide$basic, df.responses_wide$non_explanation)
cor.test(df.responses_wide$basic, df.responses_wide$non_explanation)
corr_results <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
as.matrix() |>
rcorr(type = "pearson")
rs = corr_results[["r"]]
ns = corr_results[["n"]]
ps = corr_results[["P"]]
View(ps)
library(tidyverse)
library(brms)
library(here)
library(broom)
library(Hmisc)
# constants
corpus = "katz"
corpus_set = "test"
gpt_version = "davinci"
prompt_types = c("basic", "non_explanation", "QUD_v3", "similarity", "contrast")
K = 10
temp = 0.9
df.all_responses <- data.frame()
for (prompt_type in prompt_types) {
df.responses_prompt <- read.csv(
here(sprintf("data/model-outputs/model_responses_corpus=%s-set=%s-gpt=%s-prompt=%s-k=%s-temp=%s-processed.csv",
corpus, corpus_set, gpt_version, prompt_type, K, temp)))
df.responses_prompt <- df.responses_prompt |>
mutate(prompt_type = prompt_type)
df.all_responses <- rbind(df.all_responses, df.responses_prompt)
}
df.all_responses <- df.all_responses |>
select(ID, Group, Statement, prompt_type, appropriateness_score)
df.responses_wide <- df.all_responses |>
pivot_wider(
id_cols = c("ID", "Group", "Statement"),
names_from = prompt_type,
values_from = appropriateness_score
)
cor.test(df.responses_wide$basic, df.responses_wide$non_explanation)
corr_results <- df.responses_wide |>
select(basic, non_explanation, QUD_v3, similarity, contrast) |>
as.matrix() |>
rcorr(type = "pearson")
rs = corr_results[["r"]]
ns = corr_results[["n"]]
ps = corr_results[["P"]]
View(rs)
View(rs)
View(ps)
View(df.all_responses)
View(df.responses_prompt)
